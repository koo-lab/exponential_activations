{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating synthetic sequences with embedded regulatory grammars for RNA-binding proteins\n",
    "\n",
    "In this notebook, we will generate synthetic sequences with implanted regulatory grammars, i.e. set of motifs.  Each regulatory grammar is defined by a set of motifs that are separated in space a specific distance.  In this simple dataset, only a single regulatory grammar is embedded within each sequence, which basically means that there is no useless motifs implanted, i.e. only sampling noise from the position weight matrices.  In addition, the start position of the regulatory grammar can be implanted in random locations on the seuqence, albeit the spacing between the motifs are conserved.  Details of the simulation procedure is outlined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(22) # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process JASPAR pfm and convert to pwm\n",
    "\n",
    "First we need to download the JASPAR motifs:\n",
    "\n",
    "! wget http://jaspar.genereg.net/html/DOWNLOAD/JASPAR_CORE/pfm/nonredundant/pfm_vertebrates.txt -O ../data/synthetic_TF_dataset/pfm_vertebrates.txt\n",
    "\n",
    "If this doesn't work, then you can download it manually from the link and place it in the ../data/synthetic_TF_dataset directory.\n",
    "\n",
    "\n",
    "Next, we need to load and parse the JASPAR motifs and generate a pool of core motifs for CTCF, GABPA, SP1, SRF, and YY1, and a generic pool of motifs from which we will randomly sample from.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaspar_motifs(file_path):\n",
    "    def get_motif(f):\n",
    "        line = f.readline()\n",
    "        name = line.strip().split()[1]\n",
    "        pfm = []\n",
    "        for i in range(4):\n",
    "            line = f.readline()\n",
    "            if len(line.split()[1]) > 1:\n",
    "                pfm.append(np.asarray(np.hstack([line.split()[1][1:], line.split()[2:-1]]), dtype=float))\n",
    "            else:\n",
    "                pfm.append(np.asarray(line.split()[2:-1], dtype=float))\n",
    "        pfm = np.vstack(pfm)\n",
    "        sum_pfm = np.sum(pfm, axis=0)\n",
    "        pwm = pfm/np.outer(np.ones(4), sum_pfm)\n",
    "        line = f.readline()\n",
    "        return name, pwm\n",
    "\n",
    "    num_lines = sum(1 for line in open(file_path))\n",
    "    num_motifs = int(num_lines/6)\n",
    "\n",
    "    f = open(file_path)\n",
    "    tf_names = []\n",
    "    tf_motifs = []\n",
    "    for i in range(num_motifs):\n",
    "        name, pwm = get_motif(f)\n",
    "        tf_names.append(name)\n",
    "        tf_motifs.append(pwm)\n",
    "\n",
    "    return tf_motifs, tf_names\n",
    "\n",
    "# parse JASPAR motifs\n",
    "savepath = '../../data'\n",
    "file_path = os.path.join(savepath, 'pfm_vertebrates.txt')\n",
    "motif_set, motif_names = get_jaspar_motifs(file_path)\n",
    "\n",
    "# get a subset of core motifs \n",
    "#core_names = ['SP1', 'Gabpa', 'CEBPB', 'MAX', 'YY1']\n",
    "core_names = ['SRF', 'STAT1']\n",
    "\n",
    "strand_motifs = []\n",
    "core_index = []\n",
    "for name in core_names:\n",
    "    index = motif_names.index(name)\n",
    "    strand_motifs.append(motif_set[index])\n",
    "    core_index.append(index)\n",
    "\n",
    "# generate reverse compliments\n",
    "core_motifs = []\n",
    "for pwm in strand_motifs:\n",
    "    core_motifs.append(pwm)\n",
    "    reverse = pwm[:,::-1]\n",
    "    core_motifs.append(reverse[::-1,:]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# randomly select background sequences which include core motifs\n",
    "num_background = 65        \n",
    "motif_index = np.random.permutation(len(motif_set))[0:num_background]\n",
    "motif_index = motif_index\n",
    "background_motifs = []\n",
    "for index in motif_index:\n",
    "    pwm = motif_set[index]\n",
    "    background_motifs.append(pwm)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter background motifs that look like core motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_index = [4, 5, 12, 15, 17, 18, 20, 36, 42, 60]\n",
    "motif_index = set(motif_index) - set(duplicate_index)\n",
    "\n",
    "# randomly select background sequences which include core motifs\n",
    "background_motifs = []\n",
    "for motif in core_motifs:\n",
    "    background_motifs.append(motif)\n",
    "    \n",
    "for index in list(motif_index)[:50]:\n",
    "    pwm = motif_set[index]\n",
    "    background_motifs.append(pwm)  \n",
    "    reverse = pwm[:,::-1]\n",
    "    background_motifs.append(reverse[::-1,:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation overview\n",
    "\n",
    "\n",
    "We define a regulatory grammar, $G$, as a the interactions of specific motifs spaced in specific spatial positions. Each grammar consists of a position weight matrix (PWM) with implanted motifs from which a synthetic sequence is generated from.  In this synthetic dataset, each simulated sequence is generated from a given grammar that is randomly translated, but the motifs and spatial positions between motifs within the grammar are conserved. \n",
    "\n",
    "To generate each regulatory grammar, we first create a model which consists of a subset of motifs and their spatial distances with respect to one another. We limit the pool of possible motifs to $M$ motifs to include all of the core motifs and a subset of motifs (num_motif total motifs) which are randomly sampled from the pool of JASPAR database (under the list tf_motifs).  For a given grammar, the number of motifs is determined by sampling an exponential distribution (parameterized by interaction_rate).  Then, the minimum is taken between this value and the max_motif, which imposes the constraint of a maximum number of motifs for a given grammar.  \n",
    "\n",
    "The motifs are randomly sampled from the pool of available motifs and the distance between each motif is determined by randomly sampling an exponential distribution (parameterized by distance_scale) plus a minimum distance between motifs (distance_offset). The motifs are placed along a PWM in locations determined by the distances between motifs. For simplicity, we will use a uniform distribution for the PWMs for all 'non-motif' nucleotides, i.e. $p = 1/4$.\n",
    "This constitutes the regulatory grammar model, from which synthetic sequences can be simulated from.  Moreover, each regulatory grammar is associated to a given class.\n",
    "\n",
    "\n",
    "Note that here we elect not to include alternative regulatory codes for a given class directly.  Since we are sampling from a smaller pool, we assume alternative codes will give rise to a different phenotype, specified by a different class.   Also, we do not alter the spacing between the motifs or include noisy motifs in the grammar model.  All of the noise comes from random sampling in generating the sequence and the translation of the grammar.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(core_motifs, min_interactions, max_interactions, seq_length):\n",
    "    \n",
    "    num_motif = len(core_motifs)\n",
    "    cum_dist = np.cumsum([0, 0, 0.5, 0.25, 0.17, .05, 0.3])\n",
    "    \n",
    "    # sample core motifs for each grammar\n",
    "    valid_sim = False\n",
    "    while not valid_sim:\n",
    "\n",
    "        # determine number of core motifs in a given grammar model\n",
    "        num_interactions = np.where(np.random.rand() > cum_dist)[0][-1]+2 #np.random.randint(min_interactions, max_interactions)\n",
    "\n",
    "        # randomly sample motifs\n",
    "        sim_motifs = np.random.randint(num_motif, size=num_interactions)\n",
    "        num_sim_motifs = len(sim_motifs)\n",
    "        #sim_motifs = sim_motifs[np.random.permutation(num_sim_motifs)]\n",
    "        \n",
    "        # verify that distances aresmaller than sequence length\n",
    "        distance = 0\n",
    "        for i in range(num_sim_motifs):\n",
    "            distance += core_motifs[sim_motifs[i]].shape[1]\n",
    "        if seq_length > distance > 0:\n",
    "            valid_sim = True    \n",
    "\n",
    "    # simulate distances between motifs + start \n",
    "    valid_dist = False\n",
    "    while not valid_dist:\n",
    "        remainder = seq_length - distance\n",
    "        sep = np.random.uniform(0, 2, size=num_sim_motifs+1)\n",
    "        sep = np.round(sep/sum(sep)*remainder).astype(int)\n",
    "        if np.sum(sep) == remainder:\n",
    "            valid_dist = True\n",
    "\n",
    "    # build a PWM for each regulatory grammar\n",
    "    pwm = np.ones((4,sep[0]))/4\n",
    "    for i in range(num_sim_motifs):\n",
    "        pwm = np.hstack([pwm, core_motifs[sim_motifs[i]], np.ones((4,sep[i+1]))/4])\n",
    "\n",
    "    return pwm\n",
    "\n",
    "\n",
    "def simulate_sequence(sequence_pwm):\n",
    "    \"\"\"simulate a sequence given a sequence model\"\"\"\n",
    "\n",
    "    nucleotide = 'ACGT'\n",
    "\n",
    "    # sequence length\n",
    "    seq_length = sequence_pwm.shape[1]\n",
    "\n",
    "    # generate uniform random number for each nucleotide in sequence\n",
    "    Z = np.random.uniform(0,1,seq_length)\n",
    "\n",
    "    # calculate cumulative sum of the probabilities\n",
    "    cum_prob = sequence_pwm.cumsum(axis=0)\n",
    "\n",
    "    # go through sequence and find bin where random number falls in cumulative \n",
    "    # probabilities for each nucleotide\n",
    "    one_hot_seq = np.zeros((4, seq_length))\n",
    "    for i in range(seq_length):\n",
    "        index=[j for j in range(4) if Z[i] < cum_prob[j,i]][0]\n",
    "        one_hot_seq[index,i] = 1\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation overview\n",
    "\n",
    "The number of sequences for each grammar is determined by a randomly sampled population fraction (pop_fraction).  For a each grammar model, the total number of sequences simulated is then, $N$ times its population fraction. For each sequence, the grammar model is randomly translated according to a Gaussian distribution, but with constraints to make sure the entire grammar is contained within the sequence.  Then, the cumulative sum at each nucleotide position of the translated model is calculated.  A uniform random number from 0 to 1 is generated and the bin with which it falls with respect to the the cumulative proprabilities specifies the simulated nucleotide value.  THis is done for each nucleotide position to simulate the entire sequence of length $S$. \n",
    "\n",
    "After all of the the synthetic sequences are generated, i.e. the sequence model (translated grammar), and an indicator vector of length $G$ specifying which grammar generated the sequence, the dataset is split into training, cross-validation, and a test set.  Then each dataset is stored in a hdf5 file.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset parameters\n",
    "num_seq = 20000             # number of sequences\n",
    "seq_length = 200            # length of sequence\n",
    "min_interactions = 3        # exponential rate of number of motifs for each grammar\n",
    "max_interactions = 5\n",
    "# generate sythetic sequences as a one-hot representation\n",
    "seq_pwm = []\n",
    "seq_model = []    \n",
    "num_sim = int(num_seq/2)\n",
    "for j in range(num_sim):\n",
    "    signal_pwm = generate_model(core_motifs, min_interactions, max_interactions, seq_length)\n",
    "    seq_pwm.append(simulate_sequence(signal_pwm))\n",
    "    seq_model.append(signal_pwm)\n",
    "\n",
    "# simulate a background sequence\n",
    "for j in range(num_sim):\n",
    "    background_pwm = generate_model(background_motifs, 2, max_interactions, seq_length)\n",
    "    seq_pwm.append(simulate_sequence(background_pwm))\n",
    "    seq_model.append(background_pwm)\n",
    "    \n",
    "# generate labels\n",
    "seq_label = np.vstack([np.ones((num_sim,1)), np.zeros((num_sim, 1))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train, cross-validation, and test\n",
      "Generating training data\n",
      "Generating cross-validation data\n",
      "Generating test data\n",
      "Saving to: ../../data/Synthetic_code_dataset.h5\n"
     ]
    }
   ],
   "source": [
    "def split_data(data, label, model, split_size):\n",
    "    \"\"\"split data into train set, cross-validation set, and test set\"\"\"\n",
    "    \n",
    "    def subset_data(data, label, model, sub_index):\n",
    "        \"\"\"returns a subset of the data and labels based on sub_index\"\"\"\n",
    "        \n",
    "        num_sub = len(sub_index)\n",
    "        sub_set_label = []\n",
    "        sub_set_seq = []\n",
    "        sub_set_model = []\n",
    "        for index in sub_index:\n",
    "            sub_set_seq.append([data[index]])\n",
    "            sub_set_label.append(label[index])\n",
    "            sub_set_model.append([model[index]])\n",
    "        sub_set_seq = np.vstack(sub_set_seq)\n",
    "        sub_set_label = np.vstack(sub_set_label)\n",
    "        sub_set_model = np.vstack(sub_set_model)\n",
    "    \n",
    "        return (sub_set_seq, sub_set_label, sub_set_model)\n",
    "\n",
    "    \n",
    "    # determine indices of each dataset\n",
    "    N = len(data)\n",
    "    cum_index = np.cumsum(np.multiply([0, split_size[0], split_size[1], split_size[2]],N)).astype(int) \n",
    "\n",
    "    # shuffle data\n",
    "    shuffle = np.random.permutation(N)\n",
    "\n",
    "    # training dataset\n",
    "    train_index = shuffle[range(cum_index[0], cum_index[1])]\n",
    "    cross_validation_index = shuffle[range(cum_index[1], cum_index[2])]\n",
    "    test_index = shuffle[range(cum_index[2], cum_index[3])]\n",
    "\n",
    "    # create subsets of data based on indices \n",
    "    print('Generating training data')\n",
    "    train = subset_data(data, label, model, train_index)\n",
    "\n",
    "    print('Generating cross-validation data')\n",
    "    cross_validation = subset_data(data, label, model, cross_validation_index)\n",
    "    \n",
    "    print('Generating test data')    \n",
    "    test = subset_data(data, label, model, test_index)\n",
    "    \n",
    "    return train, cross_validation, test\n",
    "\n",
    "\n",
    "def save_dataset(savepath, train, valid, test):\n",
    "    f = h5py.File(savepath, \"w\")\n",
    "    dset = f.create_dataset(\"X_train\", data=train[0], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"Y_train\", data=train[1], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"model_train\", data=train[2], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"X_valid\", data=valid[0], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"Y_valid\", data=valid[1], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"model_valid\", data=valid[2], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"X_test\", data=test[0], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"Y_test\", data=test[1], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"model_test\", data=test[2], compression=\"gzip\")\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "# split into training, cross-validation, and test sets\n",
    "print(\"Splitting dataset into train, cross-validation, and test\")\n",
    "train_size = 0.7\n",
    "cross_validation_size = 0.1\n",
    "test_size = 0.2\n",
    "split_size = [train_size, cross_validation_size, test_size]\n",
    "train, valid, test = split_data(seq_pwm, seq_label, seq_model, split_size)\n",
    "\n",
    "# save to file\n",
    "filename =  'Synthetic_code_dataset.h5' \n",
    "file_path = os.path.join(savepath, filename)\n",
    "print('Saving to: ' + file_path)\n",
    "save_dataset(file_path, train, valid, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
