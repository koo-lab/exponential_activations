{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "from tfomics import utils, explain\n",
    "import os\n",
    "results_path = os.path.join('../results/', 'synthetic_code')\n",
    "params_path = os.path.join(results_path, 'params')\n",
    "\n",
    "# load data\n",
    "data_path = '../data/Synthetic_code_dataset.h5'\n",
    "data = helper.load_dataset(data_path)\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = data\n",
    "\n",
    "# load ground truth values\n",
    "test_model = helper.load_synthetic_models(data_path, dataset='test')\n",
    "true_index = np.where(y_test[:,0] == 1)[0]\n",
    "X = x_test[true_index]\n",
    "X_model = test_model[true_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../scores/task2-cnn-deep_relu.pickle', 'rb') as f:\n",
    "    saliency_scores1 = cPickle.load(f)\n",
    "    saliency_scores2 = cPickle.load(f)\n",
    "    mut_scores1 = cPickle.load(f)\n",
    "    mut_scores2 = cPickle.load(f)\n",
    "    smoothgrad_scores = cPickle.load(f)\n",
    "    integrated_scores = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7143103195146703 0.5835167810936723\n",
      "0.6290957944684681 0.4826032419728357\n",
      "0.707869463257168 0.5725538615576774\n",
      "0.7338878872210046 0.6156494063632221\n",
      "0.7241311615241915 0.5923561278986634\n",
      "0.7319758478542823 0.6147728581453894\n",
      "0.7514230111473372 0.6434021605713198\n",
      "0.7229747297610013 0.601435499267758\n",
      "0.6772726044590964 0.5377136826155335\n",
      "0.7347275092278222 0.6139955596929729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7127668328435042"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saliency1_results = []\n",
    "for i in range(10):\n",
    "    scores = saliency_scores1[i]\n",
    "    scores = scores*X\n",
    "    roc_score, pr_score = helper.interpretability_performance(X, scores, X_model)\n",
    "    print(np.mean(roc_score), np.mean(pr_score))\n",
    "    saliency1_results.append(np.mean(roc_score))\n",
    "np.mean(saliency1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7143103173046305 0.5835166927086538\n",
      "0.6290957944684681 0.4826032419728357\n",
      "0.7078695922275203 0.5725540362952095\n",
      "0.7338879772732485 0.6156494803325534\n",
      "0.7241311415121523 0.5923563863820155\n",
      "0.7319758195763385 0.6147728467094562\n",
      "0.7514230111473372 0.6434021605713198\n",
      "0.7229746682186011 0.6014354649589202\n",
      "0.6772726044590964 0.5377136826155335\n",
      "0.7347275729266948 0.613995580882174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7127668499114088"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saliency2_results = []\n",
    "for i in range(10):\n",
    "    scores = saliency_scores2[i]\n",
    "    scores = scores*X\n",
    "    roc_score, pr_score = helper.interpretability_performance(X, scores, X_model)\n",
    "    print(np.mean(roc_score), np.mean(pr_score))\n",
    "    saliency2_results.append(np.mean(roc_score))\n",
    "np.mean(saliency2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.731209665172622 0.589213903842092\n",
      "0.6869566780128482 0.5231824707911579\n",
      "0.7505159899056425 0.6334720806922021\n",
      "0.7758299763066187 0.6808374075644102\n",
      "0.769310957838115 0.6586758533535378\n",
      "0.7670637697479853 0.6686274059811725\n",
      "0.7771962065413864 0.6907202199086515\n",
      "0.7834068811960339 0.6818949244528846\n",
      "0.7180240333023797 0.5729572280109274\n",
      "0.7847067776711406 0.6913908089707024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7544220935694772"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integrated_results = []\n",
    "for i in range(10):\n",
    "    scores = integrated_scores[i]\n",
    "    scores = scores*X\n",
    "    roc_score, pr_score = helper.interpretability_performance(X, scores, X_model)\n",
    "    print(np.mean(roc_score), np.mean(pr_score))\n",
    "    integrated_results.append(np.mean(roc_score))\n",
    "np.mean(integrated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.733606284570365 0.5014604117344308\n",
      "0.8127462926085601 0.5719206932493694\n",
      "0.8094541711113112 0.5816873731059736\n",
      "0.8249184162413605 0.6129000351579479\n",
      "0.8170858070835405 0.5915352613385599\n",
      "0.8372272148686377 0.641580004230468\n",
      "0.8528478996407998 0.6828415436654509\n",
      "0.8353526491174506 0.6422742827299166\n",
      "0.7997907874664946 0.5591275500400072\n",
      "0.8417471553551709 0.6371600785825198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.816477667806369"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saliency1_results = []\n",
    "for i in range(10):\n",
    "    scores = saliency_scores1[i]\n",
    "    scores = np.sum(scores**2, axis=-1, keepdims=True)*X\n",
    "    roc_score, pr_score = helper.interpretability_performance(X, scores, X_model)\n",
    "    print(np.mean(roc_score), np.mean(pr_score))\n",
    "    saliency1_results.append(np.mean(roc_score))\n",
    "np.mean(saliency1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.733606171781467 0.5014603719921628\n",
      "0.8127462926085601 0.5719206932493694\n",
      "0.8094541801378986 0.5816873762252401\n",
      "0.8249184623981607 0.6129000694830049\n",
      "0.8170858070835405 0.5915352613385599\n",
      "0.8372272785675102 0.6415800268045342\n",
      "0.8528479446669216 0.6828415626537397\n",
      "0.8353526491174506 0.6422742845632269\n",
      "0.7997907518910167 0.5591275142335198\n",
      "0.8417472668967543 0.6371603206106841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8164776805149281"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saliency2_results = []\n",
    "for i in range(10):\n",
    "    scores = saliency_scores2[i]\n",
    "    scores = np.sum(scores**2, axis=-1, keepdims=True)*X\n",
    "    roc_score, pr_score = helper.interpretability_performance(X, scores, X_model)\n",
    "    print(np.mean(roc_score), np.mean(pr_score))\n",
    "    saliency2_results.append(np.mean(roc_score))\n",
    "np.mean(saliency2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7253417260337885 0.5110113688838296\n",
      "0.8102257959627958 0.5808227110986295\n",
      "0.840256260131439 0.6598349484519511\n",
      "0.875166609428331 0.7179085420073102\n",
      "0.8667642334347896 0.7074738101140967\n",
      "0.8671616847046175 0.7129152786928891\n",
      "0.8801063799414559 0.750584576282361\n",
      "0.8663052891688844 0.7274779770624781\n",
      "0.7958826838377135 0.5655566615587234\n",
      "0.8925878121148282 0.760267717101881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8419798474758643"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integrated_results = []\n",
    "for i in range(10):\n",
    "    scores = integrated_scores[i]\n",
    "    scores = np.sum(scores**2, axis=-1, keepdims=True)*X\n",
    "    roc_score, pr_score = helper.interpretability_performance(X, scores, X_model)\n",
    "    print(np.mean(roc_score), np.mean(pr_score))\n",
    "    integrated_results.append(np.mean(roc_score))\n",
    "np.mean(integrated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.6045421836228287\n",
      "0.8743149284132838 0.7483184299528272\n",
      "0.8790550654040121 0.7551558518915426\n",
      "0.8889162175071956 0.7668308364308353\n",
      "0.8869033272745707 0.7649623108236271\n",
      "0.8811334535590213 0.7352373211494818\n",
      "0.8797718119445341 0.7248358073386278\n",
      "0.8856457280904709 0.7506708048715793\n",
      "0.8747049764544506 0.7485892756206604\n",
      "0.8944807764668599 0.7758806378224641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84449262851144"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mut1_results = []\n",
    "for i in range(10):\n",
    "    scores = mut_scores1[i]\n",
    "    scores = np.sum(scores**2, axis=-1, keepdims=True)*X\n",
    "    roc_score, pr_score = helper.interpretability_performance(X, scores, X_model)\n",
    "    print(np.mean(roc_score), np.mean(pr_score))\n",
    "    mut1_results.append(np.mean(roc_score))\n",
    "np.mean(mut1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86548892298477 0.7271159327825483\n",
      "0.867810334389382 0.7293930880113935\n",
      "0.8731827078788607 0.7388946873223284\n",
      "0.8852299077991433 0.7595855467775592\n",
      "0.8828863582455908 0.7574798275341235\n",
      "0.8830672024147712 0.754895431496483\n",
      "0.8847871482052222 0.7585845941362311\n",
      "0.8858414830224121 0.7634501309454996\n",
      "0.8663025408040699 0.7236466697659111\n",
      "0.8899652039772993 0.7648699416466165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8784561809721522"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mut2_results = []\n",
    "for i in range(10):\n",
    "    scores = mut_scores2[i]\n",
    "    scores = np.sum(scores**2, axis=-1, keepdims=True)*X\n",
    "    roc_score, pr_score = helper.interpretability_performance(X, scores, X_model)\n",
    "    print(np.mean(roc_score), np.mean(pr_score))\n",
    "    mut2_results.append(np.mean(roc_score))\n",
    "np.mean(mut2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
