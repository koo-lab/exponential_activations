{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import helper\n",
    "\n",
    "from tfomics import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = os.path.join('../results', 'task3')\n",
    "params_path = os.path.join(results_path, 'model_params')\n",
    "save_path = os.path.join(results_path, 'scores')\n",
    "\n",
    "\n",
    "# load data\n",
    "data_path = '../data/synthetic_code_dataset.h5'\n",
    "data = helper.load_data(data_path)\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = data\n",
    "\n",
    "# load ground truth values\n",
    "test_model = helper.load_synthetic_models(data_path, dataset='test')\n",
    "true_index = np.where(y_test[:,0] == 1)[0]\n",
    "X = x_test[true_index][:500]\n",
    "X_model = test_model[true_index][:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = ['relu', 'relu_l2', 'log_relu', 'log_relu_l2']\n",
    "score_names = ['saliency_scores']#, 'mut_scores', 'integrated_scores', 'shap_scores']\n",
    "\n",
    "num_trials = 10\n",
    "\n",
    "model_name = 'cnn-dist'\n",
    "results = {}\n",
    "for activation in activations:\n",
    "    name = model_name+'_'+activation\n",
    "    results[name] = {}\n",
    "\n",
    "    file_path = os.path.join(save_path, name+'.pickle')\n",
    "    with open(file_path, 'rb') as f:            \n",
    "        saliency_scores = cPickle.load(f)\n",
    "        #mut_scores = cPickle.load(f)\n",
    "        #integrated_scores = cPickle.load(f)\n",
    "        #shap_scores = cPickle.load(f)\n",
    "\n",
    "    all_scores = [saliency_scores]#, mut_scores, integrated_scores, shap_scores]\n",
    "\n",
    "    for score_name, scores in zip(score_names, all_scores):\n",
    "        shap_roc = []\n",
    "        shap_pr = []\n",
    "        for trial in range(num_trials):\n",
    "            if 'mut' in score_name:\n",
    "                trial_scores = np.sqrt(np.sum(scores[trial]**2, axis=-1, keepdims=True)) * X\n",
    "            else:\n",
    "                trial_scores = scores[trial] * X\n",
    "            roc_score, pr_score = helper.interpretability_performance(X, trial_scores, X_model)\n",
    "            shap_roc.append(np.mean(roc_score))\n",
    "            shap_pr.append(np.mean(pr_score))    \n",
    "\n",
    "        results[name][score_name] = [np.array(shap_roc), np.array(shap_pr)]\n",
    "        print('%s: %.4f+/-%.4f\\t'%(name+'_'+score_name, \n",
    "                                   np.mean(results[name][score_name][0]), \n",
    "                                   np.std(results[name][score_name][0]))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Log-Relu-l2', 'Log-relu', 'Relu-l2', 'Relu']\n",
    "score_name = 'saliency_scores'\n",
    "fig = plt.figure(figsize=(4,3))\n",
    "ax = plt.subplot(1,1,1)\n",
    "vals = [results['cnn-dist_log_relu_l2'][score_name][0], \n",
    "        results['cnn-dist_log_relu'][score_name][0],\n",
    "        results['cnn-dist_relu_l2'][score_name][0], \n",
    "        results['cnn-dist_relu'][score_name][0],\n",
    "       ]\n",
    "ax.boxplot(vals,  widths = 0.6);\n",
    "plt.ylabel('AUROC', fontsize=12)\n",
    "plt.yticks([0.7, 0.75, 0.8], fontsize=12)\n",
    "plt.xticks(np.linspace(1,4,4), names, fontsize=12, rotation=60)\n",
    "ax.set_ybound([.69,0.84])\n",
    "ax.set_xbound([.5,4.5])\n",
    "outfile = os.path.join(results_path, 'task3_compare_attr_score_auroc_log.pdf')\n",
    "fig.savefig(outfile, format='pdf', dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Log-Relu-l2', 'Log-relu', 'Relu-l2', 'Relu']\n",
    "\n",
    "fig = plt.figure(figsize=(4,3))\n",
    "ax = plt.subplot(1,1,1)\n",
    "vals = [results['cnn-dist_log_relu_l2'][score_name][1], \n",
    "        results['cnn-dist_log_relu'][score_name][1],\n",
    "        results['cnn-dist_relu_l2'][score_name][1], \n",
    "        results['cnn-dist_relu'][score_name][1],\n",
    "       ]\n",
    "ax.boxplot(vals,  widths = 0.6);\n",
    "plt.ylabel('AUPR', fontsize=12)\n",
    "plt.yticks([ 0.6, 0.65, 0.7], fontsize=12)\n",
    "plt.xticks(np.linspace(1,4,4), names, fontsize=12, rotation=60)\n",
    "ax.set_ybound([.58,0.74])\n",
    "ax.set_xbound([.5,4.5])\n",
    "outfile = os.path.join(results_path, 'task3_compare_attr_score_pr_log.pdf')\n",
    "fig.savefig(outfile, format='pdf', dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
